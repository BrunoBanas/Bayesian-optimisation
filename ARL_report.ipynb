{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced research lab report - Bayesian optimization for more effective search of the thermoelectic material space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The space of possible thermoelectric materials is too large to be fully explored experimentally, or even using high-throughput computational techniques. Machine learning, specifically, Bayesian optimization (BO) represents a new tool that can help scientists explore the material space and discover new high-performance materials more efficiently. \n",
    "\n",
    "BO is usually characterized by an iteretive loop:\n",
    "    1. Select an initial data set\n",
    "    2. Train a surrogate model to imitate the relationship between the features and the target property\n",
    "    3. Using an appropriate acquisition function, find a point in the (bounded) feature space which has the best balance of exploration and exploitation\n",
    "    4. Evaluate the target property at the chosen features\n",
    "    5. Append the new point to the data set, repeat until a set number of iterations is achieved\n",
    "\n",
    "This work explores the use of different methods of obtaining the surrogate models, mainly comparing the typical gaussian process (GP) to a probabilistic Bayesian neural network (pBNN). \n",
    "\n",
    "BO is the workflow of choice for the analysis of complicated, multi-dimentional data. A thermoelectic materials data set was chosen, following the work (Y.-F. Lim, C. K. Ng, K. Hippalgaonkar, 2021). The data has been obtained from high-throughput simulations for many materials at different temperatures. The target property is the power factor and the features space consists of e.g. mean covalent radius, temperature, doping or electronegativity.\n",
    "\n",
    "To test the different surrogates the BO will be repeated for each method and the averages compared. Point 4 of the BO loop is the one that in practice takes the longest, sometimes even months. To this extent, a neural network (NN) is trained on the very large data set to model the relationship between features and the target. It is used to act as a predictive oracle which replaces the experimental (computational) evaluation with a simple evaluation of the NN. \n",
    "\n",
    "The best surrogate model, which achieves the highest power-factor in the fewest number of iterations is established to be the pBNN.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all the important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from skopt import dummy_minimize\n",
    "from skopt import forest_minimize\n",
    "from skopt import gbrt_minimize\n",
    "from skopt.optimizer import base_minimize\n",
    "from NeuralEnsemble import NeuralEnsembleRegressor\n",
    "from skopt import gp_minimize\n",
    "from skopt.learning import GaussianProcessRegressor\n",
    "from skopt.learning.gaussian_process.kernels import ConstantKernel, Matern\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchbnn as bnn\n",
    "device = 'cuda:0'\n",
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preperation\n",
    "\n",
    "The data set has been prepared before by Hippalgaonkar et al., 2021. The same workflow is used here to obtain reasonable test and train sets. Transformations are performed both on the feature space and the target to make the training of the NN more effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('./cubic.xlsx')\n",
    "pipeline = Pipeline(steps=[('Scaler', StandardScaler()), ('Yeo-Johnson', PowerTransformer(method='yeo-johnson'))])\n",
    "\n",
    "def data_prep(data):\n",
    "    #data.hist(figsize =(15,15))\n",
    "    #plt.tight_layout()\n",
    "    #plt.show()\n",
    "    data['Power Factor'] = data['Power Factor'] * 1e-23\n",
    "    drop = ['index', 'nelements', 'n', 'p', 'nsites', 'direct', 'indirect']\n",
    "    df_drop = data.drop(drop, axis=1)\n",
    "    df_drop['Cbrt_PF'] = np.cbrt(df_drop['Power Factor'])\n",
    "    X = df_drop.drop(['Power Factor', 'Cbrt_PF'], axis=1)\n",
    "    Y = df_drop['Cbrt_PF']\n",
    "\n",
    "    column = X.columns \n",
    "    X, Y = shuffle(X,Y)\n",
    "    X = pipeline.fit_transform(X)\n",
    "    X = pd.DataFrame(X)\n",
    "    X.columns = column    \n",
    "    #Y = Y.values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.1, random_state=1)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = data_prep(data)\n",
    "columns = X_train.columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of predictive oracle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting function for validation of regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prediction vs true values for training and test sets - interpolation\n",
    "def plot(regressor_name, y_train, y_train_predict, y_test, y_test_predict, Mean, Std):\n",
    "\n",
    "    \n",
    "    plt.figure(figsize = (14,6))\n",
    "    plt.rc('xtick',labelsize=14)\n",
    "    plt.rc('ytick',labelsize=14)\n",
    "    '''\n",
    "    plot the predicted values against train data, calculate accuracy metrics\n",
    "    '''\n",
    "    # first subplot - training set\n",
    "    ax1 = plt.subplot(121)\n",
    "    \n",
    "    # plot predicted values vs actual values\n",
    "    plt.scatter(y_train,y_train_predict)\n",
    "    \n",
    "    # add a dash line indicating perfect predictions (Y=X)\n",
    "    plt.plot([y_train.min(),y_train.max()],[y_train.min(),y_train.max()],'r--',lw=3)\n",
    "    \n",
    "    plt.xlabel('True Value ()',fontsize=16)\n",
    "    plt.ylabel('Predicted Value ()',fontsize=16)\n",
    "    \n",
    "    # calculate mean squared error\n",
    "    mse = lambda y_train,y_train_predict: np.mean((y_train-y_train_predict)**2)\n",
    "    rmse = mse(y_train,y_train_predict)**0.5\n",
    "    \n",
    "    # calculate R2\n",
    "    R2 = metrics.r2_score(y_train, y_train_predict)\n",
    "    \n",
    "    \n",
    "    plt.fill_between(y_train, Mean-1.96*Std, \n",
    "                 Mean+1.96*Std,alpha=0.4, color=\"c\", \n",
    "                 label=\"95% Confidence Interval\")\n",
    "\n",
    "    plt.title(regressor_name+' training set'+'\\nRMSE: '+str(rmse)+'\\nR2: '+str(R2),fontsize=16)\n",
    "\n",
    "    plt.rc('xtick',labelsize=14)\n",
    "    plt.rc('ytick',labelsize=14)\n",
    "    \n",
    "    '''\n",
    "    Do the same but for the test set\n",
    "    '''\n",
    "    # second subplot - test set\n",
    "    ax2 = plt.subplot(122)\n",
    "    \n",
    "    # plot predicted values vs actual values\n",
    "    plt.scatter(y_test,y_test_predict)\n",
    "    \n",
    "    # add a dash line indicating perfect predictions (Y=X)\n",
    "    plt.plot([y_test.min(),y_test.max()],[y_test.min(),y_test.max()],'k--',lw=3)\n",
    "\n",
    "    plt.xlabel('True Value ()',fontsize=16)\n",
    "    plt.ylabel('Predicted Value ()',fontsize=16)\n",
    "    \n",
    "    # calculate mean squared error\n",
    "    mse = lambda x,y: np.mean((x-y)**2)\n",
    "    rmse = mse(y_test,y_test_predict)**0.5\n",
    "    \n",
    "    # calculate R2\n",
    "    R2 = metrics.r2_score(y_test,y_test_predict)\n",
    "    \n",
    "    plt.title(regressor_name+' test set'+'\\nRMSE: '+str(rmse)+'\\nR2: '+str(R2),fontsize=16)\n",
    "\n",
    "\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    # return mse and mape - we can track the errors across different algorithms\n",
    "    return rmse, R2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training an ensamble of NNs. This NN ensamble regressor was also prepared by Hippalgaonkar et al., 2021. The extrapolative performance of this model is of importance, since the quality of prediction around the boundary of the data is crucial. The maximum of the prediction occurs at the boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Ensemble Regressor fitted on the whole dataset\n",
    "ner = NeuralEnsembleRegressor(ensemble_size=10, max_iter=4000)\n",
    "ner.fit(X_train,y_train)\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'NNE_model_new.sav'\n",
    "pickle.dump(ner, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the prediction oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load the model from disk\n",
    "\n",
    "ner = pickle.load(open('NNE_model_new.sav', 'rb'))\n",
    "\n",
    "def prediction_neg(X_norm):  #negated output needed for the minimizing surrogates \n",
    " \n",
    "    # need to reshape for prediction\n",
    "    X_norm = np.array(X_norm)\n",
    "    X_norm = X_norm.reshape(-1,len(columns))\n",
    "\n",
    "    # the oracle does the prediction\n",
    "    pred = ner.predict(X_norm)\n",
    "\n",
    "    # need to return a scalar\n",
    "    # negative value is returned since by default the optimizer minimizes the function\n",
    "    return -pred[0]\n",
    "\n",
    "def prediction(X_norm):   \n",
    "    X_norm = np.array(X_norm)\n",
    "    X_norm = X_norm.reshape(-1,len(columns))\n",
    "    pred = ner.predict(X_norm)\n",
    "    return pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the bounds within which the search will be conducted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_values = []\n",
    "max_values = []\n",
    "\n",
    "for i in range(len(columns)):\n",
    "    min_values.append(X_train.iloc[:, i].min())\n",
    "    max_values.append(X_train.iloc[:, i].max())\n",
    "    \n",
    "bounds = np.column_stack((min_values, max_values))\n",
    "bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of surrogates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the Bayesian loop for BNN surrogate. The BNN is trained several times on the same small data set, each time it is trained, the prediction of the target value is calculated few times. The target value is predicted on a large grid of random points within the bounds. Ideally a regular grid would be used instead of the random points, however a regular grid in the high-dimentional space of any meaningful size takes up too much memory. The mean and standard deviation of the predictions are calculated and using those the acquisition function picks out the best point. \n",
    "\n",
    "Random points ensure the space is sampled representativelly, however the density of the points is very small. To increase the density of points on which we predict the target, the best point so far is picked and new grid of smaller size is defined around it. The new grid will consist of the same number of points but since the grid size is smaller, the density of points increases and with it also the probability to find a point with better exploration-exploitation trade-off. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BNN_acq import acquisition\n",
    "\n",
    "def BNN_BO(X_train, y_train, bounds, iterations_predict = 10, iterations_acq = 10, xi = 0.01):\n",
    "    y_iter=[]\n",
    "    \n",
    "    for _ in range(iterations_acq):\n",
    "\n",
    "        grid = np.random.uniform(bounds[:, 0], bounds[:, 1], size=(1000000, bounds.shape[0])) #large grid of numbers for which EI will be evaluated\n",
    "        grid = torch.tensor(grid,  dtype = dtype, device = device)\n",
    "\n",
    "        best_point = acquisition.expected_improvement_disc(grid, X_train, y_train, iterations_predict, xi) #find the index of the point with the largest EI\n",
    "\n",
    "        best_acq_tt =  grid[best_point]\n",
    "        best_acq = best_acq_tt.cpu()# point with the largest EI is selected\n",
    "        \n",
    "        #selecting a smaller region centered arounds the best point \n",
    "        lower_bound = best_acq - 0.1 # range of features is about 5, this decreases the range to 0.2\n",
    "        upper_bound = best_acq + 0.1\n",
    "\n",
    "        grid = np.random.uniform(lower_bound, upper_bound, size=(1000000, bounds.shape[0])) #building a new grid around the point with highest EI\n",
    "        grid = torch.cat((torch.tensor(grid, dtype=dtype, device=device),torch.unsqueeze(best_acq_tt.flatten(), dim=0)), dim=0)\n",
    "        \n",
    "        best_point = acquisition.expected_improvement_disc(grid, X_train, y_train, iterations_predict, xi) #find the index of the point with the largest EI\n",
    "\n",
    "        best_acq_tt =  grid[best_point]\n",
    "        best_acq = best_acq_tt.cpu()# point with the largest EI is selected\n",
    "        \n",
    "        #selecting a smaller region centered arounds the best point \n",
    "        lower_bound = best_acq - 0.01 # this decreases the range to 0.02\n",
    "        upper_bound = best_acq + 0.01\n",
    "\n",
    "        grid = np.random.uniform(lower_bound, upper_bound, size=(1000000, bounds.shape[0])) #building a new grid around the point with highest EI\n",
    "        grid = torch.cat((torch.tensor(grid, dtype=dtype, device=device),torch.unsqueeze(best_acq_tt.flatten(), dim=0)), dim=0)\n",
    "\n",
    "        best_point = acquisition.expected_improvement_disc(grid, X_train, y_train, iterations_predict, xi) #find the index of the point with the largest EI\n",
    "\n",
    "        best_acq = grid[best_point] # point with the largest EI is selected\n",
    "        best_acq = torch.unsqueeze(best_acq.flatten(), dim=0)\n",
    "        \n",
    "        # Concatenate the tensors along the first dimension (rows)\n",
    "        X_train = torch.cat((X_train, best_acq), dim=0)\n",
    "        y_train = torch.cat((y_train, torch.tensor([prediction(best_acq.cpu().numpy())], dtype = dtype, device = device)),dim=0) # evaluation of the new point with prediction oracle and adding it to training data\n",
    "\n",
    "        #print(y_train[-1])\n",
    "        y_iter.append(y_train[-1].tolist())\n",
    "    return y_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing the optimization on randomly selected initial points from within the bounds. The same initial points are used for all different surrogate methods. The optimization is repeated several times, each time with a different selection of starting points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ensemble = 3\n",
    "n_calls = 30\n",
    "n_dims = len(columns)\n",
    "\n",
    "matern_tunable = ConstantKernel(1.0, (1e-12, 1e12)) * Matern(length_scale=np.ones(n_dims), length_scale_bounds=[(1e-12, 1e12)] * n_dims, nu=2.5)\n",
    "gpregressor = GaussianProcessRegressor(kernel=matern_tunable, n_restarts_optimizer=10, alpha=1e-4, normalize_y=True, noise='gaussian')\n",
    "nneregressor = NeuralEnsembleRegressor(ensemble_size=10)\n",
    "\n",
    "res_gp = []\n",
    "res_dummy = []\n",
    "res_forest= []\n",
    "res_gbrt = []\n",
    "res_nn = []\n",
    "res_bnn = []\n",
    "\n",
    "\n",
    "for i in range(n_ensemble):\n",
    "\n",
    "    N_random_starts = 10\n",
    "    # Generate N random input points within the bounds\n",
    "    X_random = []\n",
    "    y_random = []\n",
    "    for _ in range(N_random_starts):\n",
    "        random_array = np.random.uniform(bounds[:, 0], bounds[:, 1])\n",
    "        X_random.append(random_array)\n",
    "        y = prediction(random_array)\n",
    "        y_random.append(y)\n",
    "    X_random = np.array(X_random).tolist()\n",
    "    y_random_tt = torch.tensor(np.array(y_random), dtype = dtype, device=device).flatten()    \n",
    "    X_random_tt = torch.tensor(np.array(X_random), dtype = dtype, device=device)\n",
    "    \n",
    "    res_dummy.append(dummy_minimize(prediction_neg, bounds, n_calls = n_calls, x0 = X_random, y0 = y_random))\n",
    "    res_forest.append(forest_minimize(prediction_neg, bounds, x0 = X_random, y0 = y_random, base_estimator='RF', acq_func='EI', n_calls = n_calls, xi=1.2))\n",
    "    res_gbrt.append(gbrt_minimize(prediction_neg, bounds,acq_func='EI', n_calls = n_calls,x0 = X_random, y0 = y_random))    \n",
    "    res_gp.append(gp_minimize(prediction_neg, bounds, n_calls = n_calls, base_estimator = gpregressor, acq_func='LCB', x0 = X_random, y0 = y_random, kappa=1.4))\n",
    "    res_nn.append(base_minimize(prediction_neg, bounds, n_calls = n_calls, base_estimator=nneregressor, acq_func='EI',x0 = X_random, y0 = y_random, acq_optimizer=\"sampling\", xi=0.01))    \n",
    "    \n",
    "    y_iter = BNN_BO(X_random_tt, y_random_tt, bounds, 5, n_calls, 0.01) \n",
    "    res_bnn.append(y_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of model names\n",
    "res_names = ['Dummy', 'Forest', 'Gradient Boosting', 'Gaussian Process', 'Neural Ensemble']\n",
    "res_array = [res_dummy, res_forest, res_gbrt, res_gp, res_nn]\n",
    "# Create an empty list to store legends for each line\n",
    "legends = []\n",
    "plt.figure(figsize=(10, 6),dpi=300) \n",
    "# Loop through the results and plot each line with a label\n",
    "for i, res_data in enumerate(res_array):\n",
    "    \n",
    "    df_res = pd.DataFrame({str(j): res_data[j].func_vals for j in range(len(res_data))})\n",
    "    df_res = -df_res.mean(axis=1)\n",
    "    final_df = df_res#.cummax() \n",
    "    plt.plot(range(40), final_df, label=res_names[i])  # Use the name from res_names\n",
    "    legends.append(res_names[i])\n",
    "\n",
    "# Plot other data (you may need to adjust this part according to your needs)\n",
    "df = pd.DataFrame(res_bnn).T\n",
    "df_res = -pd.DataFrame({str(j): res_forest[j].func_vals for j in range(len(res_forest))})\n",
    "\n",
    "df_mean = df.mean(axis=1)\n",
    "final_df = df_mean#.cummax()\n",
    "init_points = df_res[:10].mean(axis=1)#.cummax()\n",
    "df_iter = pd.concat([init_points, final_df], axis=0)\n",
    "\n",
    "# Plot the combined data\n",
    "plt.plot(range(40), df_iter, label=\"pBNN\", color='c')\n",
    "\n",
    "# Add a legend to the plot\n",
    "plt.legend(legends + ['pBNN'])\n",
    "plt.title('Comparing surrogates with 10 random startig points (bounded), averaged over 3 repetitions')\n",
    "plt.xlabel('Acquisition')\n",
    "plt.ylabel('Power Factor')\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The comparison of the point acquisition by different surrogates shows a clear distinction between the pBNN developed in this work and the other surrogate methods. Using pBNN, the early acquisitions, within 30 iterations (corresponding to 30 new theoretical materials) have substentially higher values of the power factor. This means the pBNN is able to easily locate local maxima, with only few points to learn from.\n",
    "\n",
    "Additional tests show that with an increasing size of initial point set, the pBNN is able to find higher values within the first acquisition. The next best alternative, gaussian process, stops being a viable option after a few hundered initial points on such highdimentional data as it requires too much memory. \n",
    "\n",
    "At larger iterations of the acquisition process (100 iterations), the gaussian process performs best as it is better at identifying the absolute maximum. This implies it would be best to combine the the pBNN with the gaussian process in a workflow where many iterations of the acquisition can be performed. The pBNN would locate several local maxima which could be used as the inital data set for the gaussian process to obtain the global maximum.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pBNN point acquisition from a discrete data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting a representative initial data set by clustering the data\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X_train_np = X_train.numpy()\n",
    "y_train_np = y_train.numpy()\n",
    "\n",
    "# Determine the number of clusters\n",
    "n_clusters = 5\n",
    "\n",
    "# Apply K-means clustering\n",
    "#To ensure diverse clusters, you can use techniques such as K-means++ initialization or specify the \"init\" parameter in scikit-learn's K-means implementation to obtain diverse initial centroids. \n",
    "#Additionally, you can consider using the \"n_init\" parameter to perform multiple random initializations and select the clustering result with the lowest inertia (sum of squared distances within clusters) to ensure a good solution.\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, init='k-means++', n_init=10)\n",
    "\n",
    "kmeans.fit(X_train_np)\n",
    "\n",
    "# Obtain cluster labels for the data points\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Access cluster centroids\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "# Select one point from the center of each cluster\n",
    "initial_X = []\n",
    "initial_y = []\n",
    "\n",
    "N = 1 #adds N x n_cluster extra random points\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    cluster_X = X_train_np[cluster_labels == i]\n",
    "    cluster_Y = y_train_np[cluster_labels == i]\n",
    "\n",
    "    # Randomly select N points from the cluster\n",
    "    selected_indices = np.random.choice(len(cluster_X), size=N, replace=False)\n",
    "    selected_X = cluster_X[selected_indices]\n",
    "    selected_Y = cluster_Y[selected_indices]\n",
    "\n",
    "    initial_X.extend(selected_X)\n",
    "    initial_y.extend(selected_Y)\n",
    "\n",
    "    # Find the center of each cluster to take as the initial starting point - most representative sample of the data\n",
    "    cluster_center = cluster_centers[i]\n",
    "    closest_point_index = np.argmin(np.linalg.norm(cluster_X - cluster_center, axis=1))\n",
    "    initial_X.append(cluster_X[closest_point_index])\n",
    "    initial_y.append(cluster_Y[closest_point_index])\n",
    "  \n",
    "    \n",
    "initial_X = torch.tensor(np.array(initial_X), dtype = dtype)\n",
    "initial_y = torch.tensor(np.array(initial_y), dtype = dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BNN_BO(X_train, y_train, bounds, iterations_predict = 10, iterations_acq = 10, xi = 0.01):\n",
    "    y_iter=[]\n",
    "    \n",
    "    for _ in range(iterations_acq):\n",
    "\n",
    "        grid = X_train_np #large grid of numbers for which EI will be evaluated\n",
    "        best_point = acquisition.expected_improvement_disc(grid, X_train, y_train, iterations_predict, xi) #find the index of the point with the largest EI\n",
    "\n",
    "        best_acq = grid[best_point] # point with the largest EI is selected\n",
    "        best_acq = torch.unsqueeze(best_acq.flatten(), dim=0)\n",
    "        \n",
    "        # Concatenate the tensors along the first dimension (rows)\n",
    "        X_train = torch.cat((X_train, best_acq), dim=0)\n",
    "        y_train = torch.cat((y_train, torch.tensor([prediction(best_acq.cpu().numpy())], dtype = dtype, device = device)),dim=0) # evaluation of the new point with prediction oracle and adding it to training data\n",
    "\n",
    "        #print(y_train[-1])\n",
    "        y_iter.append(y_train[-1].tolist())\n",
    "    return y_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_loss = nn.CrossEntropyLoss()\n",
    "mse_loss = nn.MSELoss()\n",
    "kl_loss = bnn.BKLLoss(reduction='mean', last_layer_only=False)\n",
    "kl_weight = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "def ei_acquisition(mean, std, best_value, xi=0.01):\n",
    "    z = (mean - best_value-xi) / std\n",
    "    ei = (mean - best_value) * norm.cdf(z) + std * norm.pdf(z)\n",
    "    best_index = np.argmax(ei)\n",
    "    best_point = best_index.item()\n",
    "    return best_point#, best_index\n",
    "\n",
    "def lcb_acquisition(mean, std, kappa):\n",
    "    lcb = mean - kappa * std\n",
    "    best_index = np.argmin(lcb)\n",
    "    best_point = best_index.item()\n",
    "    return best_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the Bayesian optimization loop\n",
    "\n",
    "def BNN_BO(X_init, y_init, iterations_train = 1000, iterations_predict = 15, iterations_acq = 10):\n",
    "    y_iter=[]\n",
    "    X_iter=[]\n",
    "    for _ in range(iterations_acq):\n",
    "        \n",
    "        model_bnn = nn.Sequential(\n",
    "        bnn.BayesLinear(prior_mu=0, prior_sigma=0.01, in_features=len(X_train[0]), out_features=400),\n",
    "        nn.ReLU(),\n",
    "        bnn.BayesLinear(prior_mu=0, prior_sigma=0.01, in_features=400, out_features=500),\n",
    "        nn.ReLU(),\n",
    "        bnn.BayesLinear(prior_mu=0, prior_sigma=0.01, in_features=500, out_features=1),\n",
    "        nn.Flatten(0,1)\n",
    "        )\n",
    "        y_grid = []\n",
    "        optimizer = torch.optim.RMSprop(model_bnn.parameters(), lr=0.0004)\n",
    "      \n",
    "        for _ in range(int(iterations_predict)): # retrain the model multiple times on the same dataset\n",
    "            \n",
    "            #train the model on the few initial points \n",
    "            for step in range(iterations_train):\n",
    "                    pre = model_bnn(X_init)\n",
    "                    mse = mse_loss(pre, y_init)\n",
    "                    kl = kl_loss(model_bnn)\n",
    "                    cost = mse + kl_weight*kl\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    cost.backward()\n",
    "                    optimizer.step()\n",
    "            print('- MSE : %2.2f, KL : %2.2f' % (mse.item(), kl.item()))\n",
    "\n",
    "            # Using the trained model we can predict the magnetization for the whole data set\n",
    "\n",
    "            # Perform multiple iterations to obtain set of predictions\n",
    "            for _ in range(10):\n",
    "                y_predictions = model_bnn(X_train0)#pass the whole training data set to the model to obtain a prediciton of y_train0\n",
    "\n",
    "                # Reshape the predictions tensor to match the grid structure\n",
    "                y_predictions = y_predictions.view(len(X_train0), -1)\n",
    "\n",
    "                # Convert the predictions tensor to a nested list\n",
    "                y_predictions = y_predictions.tolist()\n",
    "\n",
    "                # Append the predictions to the list\n",
    "                y_grid.append(y_predictions)\n",
    "        \n",
    "        # Convert the list of predictions to a PyTorch tensor\n",
    "        predictions_tensor = torch.tensor(y_grid)\n",
    "\n",
    "        # Calculate the mean and standard deviation along the iterations axis\n",
    "        mean = predictions_tensor.mean(dim=0)\n",
    "        std = predictions_tensor.std(dim=0)\n",
    "        \n",
    "        # use the acquisition function to predict the best point to sample from the data set\n",
    "        best_point= ei_acquisition(mean, std, y_init.max())\n",
    "        best_X = torch.unsqueeze(X_train0[best_point], dim=0) #X value at the best acquisition point \n",
    "        best_y = torch.unsqueeze(y_train0[best_point], dim=0) #y value at the best acquisition point \n",
    "\n",
    "        \n",
    "        # Make a plot of the acquisition process \n",
    "        sorted_indices = torch.argsort(X_train0[:, 0]) #sort the indecies for plotting of 1 feature against magnetization\n",
    "        sorted_X1 = X_train0[:, 0][sorted_indices]\n",
    "        sorted_mean = mean[sorted_indices]\n",
    "        sorted_y = y_train0[sorted_indices]\n",
    "        sorted_std = std[sorted_indices]\n",
    "        \n",
    "        plt.scatter(X_init[:,0], y_init, color = 'green',zorder=2, label = 'initial points + old acq')\n",
    "        plt.scatter(best_X[:,0], best_y, color = 'red', zorder=3, label = 'current acq')\n",
    "        plt.plot(sorted_X1, sorted_mean, color = 'blue', zorder=1, label= 'mean')\n",
    "        \n",
    "        lower_bound = sorted_mean - 1.96 * sorted_std  # 1.96 is the z-score for a 95% confidence interval\n",
    "        upper_bound = sorted_mean + 1.96 * sorted_std\n",
    "        \n",
    "        # Fill the area between the upper and lower bounds with a translucent color\n",
    "        plt.fill_between(sorted_X1, lower_bound.flatten(), upper_bound.flatten(), color='lightgreen', alpha=0.9,zorder=0, label='95% confidance interval')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.xlabel('Transformed mean Effective Coordination')\n",
    "        plt.ylabel('Transformed Magnetization')\n",
    "        plt.show()\n",
    "        \n",
    "        X_init = torch.cat((X_init, best_X), dim=0)\n",
    "        y_init = torch.cat((y_init, best_y),dim=0)\n",
    "        print(best_y)\n",
    "        y_iter.append(y_init[-1].tolist())\n",
    "        X_iter.append(X_init[-1].tolist())\n",
    "        \n",
    "        \n",
    "    return y_iter, X_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization \n",
    "\n",
    "For the sklearn surrogates the hyperparameters have been optimized such as what follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn import metrics\n",
    "def objective(trial):\n",
    "    acq = []\n",
    "    #x = trial.suggest_float('x', 1.1, 1.8)\n",
    "    #y = trial.suggest_float('y', 0.1, 0.4, log=True)\n",
    "    y = trial.suggest_float('y', 0.05, 0.25)\n",
    "    n_calls = 30\n",
    "\n",
    "    # Number of repetitions to get the optimization statistics.\n",
    "    n_ensemble = 3\n",
    "\n",
    "    n_dims = len(columns)\n",
    "\n",
    "    matern_tunable = ConstantKernel(1, (1e-12, 1e12)) * Matern(\n",
    "        length_scale=np.ones(n_dims), length_scale_bounds=[(1e-12, 1e12)] * n_dims, nu=2.5)\n",
    "\n",
    "\n",
    "    res_gp = []\n",
    "    for i in range(n_ensemble):\n",
    "        \n",
    "        gpregressor = GaussianProcessRegressor(kernel=matern_tunable, n_restarts_optimizer=10, \n",
    "                                            alpha=y, normalize_y=True, noise='gaussian')\n",
    "        \n",
    "        res_gp.append(gp_minimize(prediction, \n",
    "                        bounds,\n",
    "                        n_calls = n_calls, \n",
    "                        base_estimator=gpregressor,\n",
    "                        acq_func='LCB', \n",
    "                        #random_state = 1, leave random state seed random, since ensamble is used, each hyperparameter selection is averaged over 3 trails - more representative \n",
    "                        n_random_starts=1,\n",
    "                        #xi=0.01\n",
    "                        kappa=1.35))\n",
    "\n",
    "        #Transforms back to original values\n",
    "        X_reshape = np.array(res_gp[i].x).reshape(-1,len(columns))\n",
    "        X_original = pipeline.inverse_transform(X_reshape)\n",
    "    res = res_gp\n",
    "    method = ['Gaussian']\n",
    "#code from downstairs put thru gpt to make more efficient during iteretive hyperparameter opt. 40% decrease in comp time\n",
    "    df_res = pd.DataFrame({str(i): res[i].func_vals for i in range(len(res))}) \n",
    "    df_min = pd.DataFrame({str(i): np.min(df_res.iloc[:j+1, i]) for i in range(df_res.shape[1])} for j in range(len(df_res)))\n",
    "    df_power = pd.DataFrame(-df_min.min(axis=1), columns=[method])\n",
    "    return -df_power.max()\n",
    "\n",
    "'''\n",
    "list_1 = np.linspace(0.001, 0.1, 100)\n",
    "list_2 = np.log(np.linspace(1.00001,1.1 , 1000000)) #adjust to log wise sampling - samples log dist.\n",
    "search_space = {\n",
    "    #'x': list_1,\n",
    "    'y': list_2,\n",
    "}\n",
    "'''\n",
    "\n",
    "study_gp= optuna.create_study()\n",
    "study_gp.optimize(objective, n_trials=30)\n",
    "\n",
    "#study_gp = optuna.create_study(sampler=optuna.samplers.GridSampler(search_space))\n",
    "#study_gp.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_x = []\n",
    "hyperparameter_y = []\n",
    "objective_values = []\n",
    "\n",
    "for trial in study_gp.trials:\n",
    "    hyperparameter_x.append(trial.params['x'])\n",
    "    hyperparameter_y.append(trial.params['y'])\n",
    "    objective_values.append(trial.value)\n",
    "'''  \n",
    "x1 = []\n",
    "y1 = []\n",
    "x2 = []\n",
    "y2 = []\n",
    "for trial in study_gp.trials:\n",
    "    if trial.params['y'] == 1.5:\n",
    "        x1.append(trial.params['x'])\n",
    "        y1.append(trial.value)\n",
    "    else: \n",
    "        x2.append(trial.params['x'])\n",
    "        y2.append(trial.value)\n",
    "'''  \n",
    "\n",
    "# Plot the hyperparameter values against the objective values\n",
    "plt.scatter(hyperparameter_y, objective_values, label='Objective Values')\n",
    "plt.scatter(x1, y1, label='nu = 1.5')\n",
    "plt.scatter(x2, y2, label='nu = 2.5')\n",
    "plt.xlabel('alpha')  # Replace with the name of the hyperparameter\n",
    "plt.ylabel('Objective Value')\n",
    "plt.title('Hyperparameter Optimization - GP')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "hyperparameter_x1 = []\n",
    "hyperparameter_x2 = []\n",
    "objective_values = []\n",
    "\n",
    "for trial in study_gp.trials:\n",
    "    #hyperparameter_x1.append(trial.params['x'])\n",
    "    hyperparameter_x2.append(trial.params['y'])\n",
    "    objective_values.append(trial.value)\n",
    "\n",
    "def moving_average(data, window_size):\n",
    "    cumsum = np.cumsum(data, dtype=float)\n",
    "    cumsum[window_size:] = cumsum[window_size:] - cumsum[:-window_size]\n",
    "    return cumsum[window_size - 1:] / window_size\n",
    "\n",
    "def plot(hyperparameter):\n",
    "    \n",
    "    x = np.array(hyperparameter)\n",
    "    y = np.array(objective_values)\n",
    "\n",
    "    # Get indices that would sort the x array\n",
    "    sorted_indices = np.argsort(x, kind='mergesort')\n",
    "\n",
    "    # Sort both arrays based on the sorted indices\n",
    "    sorted_x = x[sorted_indices]\n",
    "    sorted_y = y[sorted_indices]\n",
    "\n",
    "    # Calculate moving average\n",
    "    window_size = 5\n",
    "    moving_avg = moving_average(sorted_y, window_size)\n",
    "\n",
    "    # Plot scatter plot and moving average\n",
    "    plt.scatter(x, y, label='Data')\n",
    "    plt.plot(sorted_x[window_size - 1:], moving_avg, color='red', label='Moving Average')\n",
    "    plt.xlabel('alpha')\n",
    "    plt.ylabel('Y')\n",
    "    plt.legend()\n",
    "    plt.title('Scatter plot with Moving Average - random seed, ensemble = 3')\n",
    "    # Set x-axis to logarithmic scale\n",
    "    plt.xscale('log')\n",
    "    return plt.show()\n",
    "\n",
    "plot(hyperparameter_x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the minimization process projected on a 2D plane:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dims = len(bounds)\n",
    "\n",
    "matern_tunable = ConstantKernel(1.0, (1e-12, 1e12)) * Matern(\n",
    "    length_scale=np.ones(n_dims), length_scale_bounds=[(1e-12, 1e12)] * n_dims, nu=2.5)\n",
    "\n",
    "gpregressor = GaussianProcessRegressor(kernel=matern_tunable, n_restarts_optimizer=10, \n",
    "                                       alpha=0.06, normalize_y=True, noise='gaussian')\n",
    "\n",
    "res = gp_minimize(prediction, \n",
    "                  bounds,\n",
    "                  n_calls=100, \n",
    "                  base_estimator=gpregressor,\n",
    "                  acq_func='LCB',\n",
    "                  n_random_starts=10, \n",
    "                  kappa=1.4)\n",
    "\n",
    "# Extract the iterations and corresponding x values\n",
    "iterations = res.x_iters\n",
    "x_values = np.array(iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing the grid\n",
    "twoD = [np.linspace(bounds[i, 0], bounds[i, -1], 100) for i in [0,1]]\n",
    "X_grid = np.meshgrid(*twoD)\n",
    "result_grid = np.empty_like(X_grid[0])\n",
    "\n",
    "# Iterate over the indices of X_grid arrays\n",
    "for i in range(X_grid[0].shape[0]):\n",
    "    for j in range(X_grid[0].shape[1]):\n",
    "        # Get the paired elements from X_grid arrays\n",
    "        x = np.array([X_grid[0][i, j], X_grid[1][i, j]])\n",
    "        \n",
    "        # Reshape the x array to a 2D array with one sample and two features\n",
    "        x = x.reshape(1, -1)\n",
    "        numbers_to_append = np.array([x_values[-1, i] for i in range(2, 22)]) # Adding the numbers corresponding to the other dimentions \n",
    "\n",
    "        x = np.append(x, [numbers_to_append], axis=1)\n",
    "        # Call ner_pca.predict with the reshaped input\n",
    "        result = ner.predict(x)\n",
    "        \n",
    "        # Store the result in the corresponding position of result_grid\n",
    "        result_grid[i, j] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the function and the iterations\n",
    "plt.figure(figsize=(8, 6))\n",
    "contour = plt.contourf(twoD[0], twoD[1], result_grid, levels=100, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.scatter(x_values[-1, 0], x_values[-1, 1], c='red', linewidths=3, alpha=1, marker ='x')\n",
    "plt.plot(x_values[:, 0], x_values[:, 1], '-o', markersize=4, linewidth=1, color='black')\n",
    "\n",
    "for i in range(len(x_values)):\n",
    "    alpha = (i + 1) / len(x_values)  # Scale alpha based on the order of iteration\n",
    "    plt.plot(x_values[i,0], x_values[i,1], 'wo', markersize=3, alpha=alpha)\n",
    "plt.xlabel('Atomic weight')\n",
    "plt.ylabel('Covalent radius')\n",
    "plt.title('Minimization using gp_minimize')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## references\n",
    "\n",
    "http://krasserm.github.io/2018/03/21/bayesian-optimization/\n",
    "\n",
    "http://krasserm.github.io/2019/03/14/bayesian-neural-networks/\n",
    "\n",
    "https://github.com/Harry24k/bayesian-neural-network-pytorch/tree/master\n",
    "\n",
    "https://towardsdatascience.com/why-you-should-use-bayesian-neural-network-aaf76732c150#:~:text=What%20is%20Bayesian%20Neural%20Network,that%20best%20fit%20the%20data\n",
    "\n",
    "https://www.authorea.com/doi/full/10.22541/au.162679475.54895263/v1\n",
    "\n",
    "http://krasserm.github.io/2018/03/19/gaussian-processes/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
